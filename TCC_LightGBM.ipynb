{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eda94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==============================\n",
    "# 1. Carregar e preparar os dados\n",
    "# ==============================\n",
    "df = pd.read_csv(\"Data/DataProcessed.csv\").sort_values(\"AccountId\")\n",
    "df['OrderClassification'] = df['OrderClassification'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e796413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProductLine'] = df['ProductLine'].astype('category')\n",
    "df['Size'] = df['Size'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f1b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label: HasOrder (0, 1, 2)\n",
    "y = df[\"OrderClassification\"]\n",
    "\n",
    "# # Features atuais + possibilidade de novas features\n",
    "# feature_cols = [\n",
    "#     #'TotalPriceCurrentMonth',\n",
    "#     'TotalPrice3Month',\n",
    "#     'TotalPrice6Month',\n",
    "#     'TotalPriceCurrentYear',\n",
    "#     'NetValue',\n",
    "#     'Quantity',\n",
    "#     #'TotalQuantityCurrentMonth',\n",
    "#     'TotalQuantity3Month',\n",
    "#     'TotalQuantity6Month',\n",
    "#     'TotalQuantityCurrentYear',\n",
    "#     'AverageTicket',\n",
    "#     #'AverageTicketCurrentMonth',\n",
    "#     'AverageTicket3Month',\n",
    "#     'AverageTicket6Month',\n",
    "#     'AverageTicketCurrentYear',\n",
    "#     #'Recency',\n",
    "#     'Weight',\n",
    "#     'ProductLine',\n",
    "#     'Size',\n",
    "#     'ProductPopularity',\n",
    "#     'TotalPriceByAccount',\n",
    "#     'TotalQuantityByAccount',\n",
    "#     'RelativePriceProduct',\n",
    "#     'RelativePriceAccount'\n",
    "# ]\n",
    "feature_cols = [\n",
    "    'NetValue', \n",
    "    'Quantity',\n",
    "    'Recency', \n",
    "    'Weight', \n",
    "    'ProductLine', \n",
    "    'Size', \n",
    "    'TotalPrice3Month',\n",
    "    'TotalPrice6Month', \n",
    "    'TotalQuantity3Month', \n",
    "    'TotalQuantity6Month',\n",
    "    'AverageTicket', \n",
    "    'AverageTicket3Month',\n",
    "    'AverageTicket6Month', \n",
    "    'ProductPopularity', \n",
    "    'TotalPriceByAccount',\n",
    "    'TotalQuantityByAccount', \n",
    "    'RelativePriceProduct',\n",
    "    'RelativePriceAccount'\n",
    "]\n",
    "\n",
    "X = df[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caecebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 2. Split por AccountId (sem vazar info)\n",
    "# ==============================\n",
    "unique_accounts = df[\"AccountId\"].unique()\n",
    "train_ids, test_ids = train_test_split(unique_accounts, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df[df[\"AccountId\"].isin(train_ids)]\n",
    "test_df  = df[df[\"AccountId\"].isin(test_ids)]\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[\"OrderClassification\"].astype(int)\n",
    "X_test, y_test   = test_df[feature_cols],  test_df[\"OrderClassification\"].astype(int)\n",
    "\n",
    "# Grupos por conta (cada cliente é um grupo de produtos)\n",
    "train_groups = train_df.groupby(\"AccountId\").size().to_list()\n",
    "test_groups  = test_df.groupby(\"AccountId\").size().to_list()\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "test_data  = lgb.Dataset(X_test, label=y_test, group=test_groups, reference=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db24af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3. Hiperparâmetros otimizados\n",
    "# ==============================\n",
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"ndcg_eval_at\": [5, 10],   # Avaliação no top-5 e top-10\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 1024,\n",
    "    #\"min_data_in_leaf\": 20, #Valor Mínimo de dados por folha, para evitar overfitting\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    #'early_stopping_rounds':2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3243\n",
      "[LightGBM] [Info] Number of data points in the train set: 75741, number of used features: 18\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 4. Treinamento\n",
    "# ==============================\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    num_boost_round=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b801381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5. Predição e Ranking em um cliente específico\n",
    "# ==============================\n",
    "account_id = '001U400000QqZScIAN'\n",
    "\n",
    "X_new = df[df['Id'] == account_id][feature_cols]\n",
    "product_ids = df[df['Id'] == account_id]['ProductCode'].to_list()\n",
    "\n",
    "scores = model.predict(X_new)\n",
    "\n",
    "ranking = sorted(zip(product_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nRanking de produtos recomendados para Account {account_id}:\")\n",
    "for prod, score in ranking:\n",
    "    print(f\"Produto {prod} - score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ab262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 6. Avaliação completa no conjunto de teste\n",
    "# =========================================================\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Fazer predições para todo o conjunto de teste\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Adicionar as predições de volta ao dataframe de teste para facilitar o agrupamento\n",
    "test_df['predictions'] = test_predictions\n",
    "\n",
    "# Lista para armazenar o NDCG de cada grupo (cliente)\n",
    "ndcg_scores_k5 = []\n",
    "ndcg_scores_k10 = []\n",
    "\n",
    "# Iterar sobre cada cliente no conjunto de teste\n",
    "for account_id in test_df['Id'].unique():\n",
    "    # Filtrar os dados do cliente atual\n",
    "    client_data = test_df[test_df['Id'] == account_id]\n",
    "    \n",
    "    # Pegar os scores de relevância verdadeiros e os previstos\n",
    "    true_relevance = client_data['OrderClassification'].values.reshape(1, -1)\n",
    "    predicted_scores = client_data['predictions'].values.reshape(1, -1)\n",
    "    \n",
    "    # Ignorar clientes com apenas itens irrelevantes (NDCG é indefinido)\n",
    "    if np.sum(true_relevance) > 0:\n",
    "        # Calcular NDCG @ 5 e @ 10\n",
    "        ndcg_k5 = ndcg_score(true_relevance, predicted_scores, k=5)\n",
    "        ndcg_k10 = ndcg_score(true_relevance, predicted_scores, k=10)\n",
    "        \n",
    "        ndcg_scores_k5.append(ndcg_k5)\n",
    "        ndcg_scores_k10.append(ndcg_k10)\n",
    "\n",
    "# Calcular a média final do NDCG\n",
    "mean_ndcg_k5 = np.mean(ndcg_scores_k5)\n",
    "mean_ndcg_k10 = np.mean(ndcg_scores_k10)\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"Avaliação Final no Conjunto de Teste\")\n",
    "print(f\"NDCG médio @ 5: {mean_ndcg_k5:.4f}\")\n",
    "print(f\"NDCG médio @ 10: {mean_ndcg_k10:.4f}\")\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# ==============================\n",
    "# 1. Importância das Features\n",
    "# ==============================\n",
    "lgb.plot_importance(model, max_num_features=15, importance_type=\"gain\")\n",
    "plt.title(\"Importância das Features (LightGBM)\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 2. Explicação com SHAP\n",
    "# ==============================\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Gráfico resumido: mostra as variáveis que mais influenciam no ranking\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
